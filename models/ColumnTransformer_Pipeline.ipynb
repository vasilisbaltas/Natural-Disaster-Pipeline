{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==0.24.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/ef/bcd79e8d59250d6e8478eb1290dc6e05be42b3be8a86e3954146adbc171a/scikit_learn-0.24.2-cp36-cp36m-manylinux1_x86_64.whl (20.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 20.0MB 803kB/s eta 0:00:01  6% |██                              | 1.3MB 13.2MB/s eta 0:00:02    11% |███▉                            | 2.4MB 12.4MB/s eta 0:00:02    14% |████▊                           | 2.9MB 12.0MB/s eta 0:00:02    17% |█████▋                          | 3.5MB 12.8MB/s eta 0:00:02    28% |█████████▏                      | 5.7MB 12.3MB/s eta 0:00:02    31% |██████████                      | 6.2MB 12.7MB/s eta 0:00:02    33% |██████████▉                     | 6.8MB 17.1MB/s eta 0:00:01    36% |███████████▊                    | 7.3MB 11.1MB/s eta 0:00:02    39% |████████████▋                   | 7.8MB 11.9MB/s eta 0:00:02    41% |█████████████▍                  | 8.4MB 12.0MB/s eta 0:00:01    44% |██████████████▎                 | 8.9MB 10.4MB/s eta 0:00:02    49% |███████████████▉                | 9.9MB 10.8MB/s eta 0:00:01    51% |████████████████▌               | 10.3MB 8.2MB/s eta 0:00:02    54% |█████████████████▍              | 10.9MB 10.6MB/s eta 0:00:01    57% |██████████████████▎             | 11.4MB 10.1MB/s eta 0:00:01    66% |█████████████████████▍          | 13.4MB 11.5MB/s eta 0:00:01    69% |██████████████████████▏         | 13.9MB 11.0MB/s eta 0:00:01    71% |███████████████████████         | 14.4MB 9.1MB/s eta 0:00:01    74% |███████████████████████▊        | 14.8MB 12.8MB/s eta 0:00:01    76% |████████████████████████▌       | 15.3MB 10.3MB/s eta 0:00:01    79% |█████████████████████████▍      | 15.8MB 10.9MB/s eta 0:00:01    83% |██████████████████████████▉     | 16.8MB 11.1MB/s eta 0:00:01    91% |█████████████████████████████▍  | 18.4MB 11.6MB/s eta 0:00:01    96% |███████████████████████████████ | 19.3MB 9.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.13.3 (from scikit-learn==0.24.2)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/b2/6c7545bb7a38754d63048c7696804a0d947328125d81bf12beaa692c3ae3/numpy-1.19.5-cp36-cp36m-manylinux1_x86_64.whl (13.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 13.4MB 1.2MB/s ta 0:00:011  4% |█▌                              | 624kB 10.3MB/s eta 0:00:02    12% |████                            | 1.6MB 11.8MB/s eta 0:00:01    15% |█████▏                          | 2.1MB 9.7MB/s eta 0:00:02    19% |██████▍                         | 2.7MB 10.8MB/s eta 0:00:01    27% |████████▊                       | 3.6MB 11.7MB/s eta 0:00:01    30% |█████████▉                      | 4.1MB 12.1MB/s eta 0:00:01    34% |███████████                     | 4.6MB 10.3MB/s eta 0:00:01    37% |████████████▏                   | 5.1MB 8.7MB/s eta 0:00:01    41% |█████████████▏                  | 5.5MB 8.5MB/s eta 0:00:01    48% |███████████████▋                | 6.5MB 11.4MB/s eta 0:00:01    59% |███████████████████             | 8.0MB 12.3MB/s eta 0:00:01    70% |██████████████████████▋         | 9.5MB 11.5MB/s eta 0:00:01    80% |█████████████████████████▉      | 10.8MB 9.3MB/s eta 0:00:01    84% |███████████████████████████     | 11.3MB 11.4MB/s eta 0:00:01    90% |█████████████████████████████   | 12.1MB 8.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.24.2) (1.2.1)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn==0.24.2)\n",
      "  Downloading https://files.pythonhosted.org/packages/61/cf/6e354304bcb9c6413c4e02a747b600061c21d38ba51e7e544ac7bc66aecc/threadpoolctl-3.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.24.2) (0.11)\n",
      "\u001b[31mtensorflow 1.3.0 requires tensorflow-tensorboard<0.2.0,>=0.1.0, which is not installed.\u001b[0m\n",
      "Installing collected packages: numpy, threadpoolctl, scikit-learn\n",
      "  Found existing installation: numpy 1.12.1\n",
      "    Uninstalling numpy-1.12.1:\n",
      "      Successfully uninstalled numpy-1.12.1\n",
      "  Found existing installation: scikit-learn 0.19.1\n",
      "    Uninstalling scikit-learn-0.19.1:\n",
      "      Successfully uninstalled scikit-learn-0.19.1\n",
      "Successfully installed numpy-1.19.5 scikit-learn-0.24.2 threadpoolctl-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn==0.24.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 0.24.2.\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from Custom_Transformers import MessageTransformer, MessageTransformer_2\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///Vasilis_db.db')\n",
    "df = pd.read_sql_table('Emergency_Messages', engine)\n",
    "df = df.drop(columns = ['id','original'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with NaN messages\n",
    "df = df.dropna(subset=['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.related==2, 'related'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['message','genre']]\n",
    "Y = df.drop(columns=['message','genre'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\" Function that tokenizes and lemmatizes text\n",
    "\n",
    "    :param text:     input text to be processed(str)\n",
    "    :return:         cleaned_tokens(str)\n",
    "    \"\"\"\n",
    "\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Text normalization\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    #tokenization\n",
    "    words = word_tokenize (text)\n",
    "    # lemmatization\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(word).strip() for word in words if word not in stop_words]\n",
    "\n",
    "    return cleaned_tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transformer = Pipeline([\n",
    "    ('messagetransformer', MessageTransformer_2(tokenize)),\n",
    "])\n",
    "genre_transformer = Pipeline([\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('text', text_transformer, [0]),\n",
    "    ('genre', genre_transformer, [1])\n",
    "])\n",
    "pipeline_2 = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('clf', RandomForestClassifier(random_state=33))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(transformers=[('text',\n",
       "                                 Pipeline(steps=[('messagetransformer',\n",
       "                                                  MessageTransformer_2(tokenize=<function tokenize at 0x7f8a309680d0>))]),\n",
       "                                 [0]),\n",
       "                                ('genre',\n",
       "                                 Pipeline(steps=[('onehot',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                 [1])])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.fit(X_train.iloc[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 4)\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0. 0.]\n",
      " [1. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "transformed_test_1 = preprocessor.transform(X_train.iloc[:2000])\n",
    "print(transformed_test_1.shape)\n",
    "print(transformed_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 4)\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "transformed_test_2 = preprocessor.transform(X_train.iloc[2000:8000])\n",
    "print(transformed_test_2.shape)\n",
    "print(transformed_test_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
